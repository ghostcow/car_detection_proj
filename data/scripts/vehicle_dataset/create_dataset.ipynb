{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import cPickle as pkl\n",
    "\n",
    "DATASET_VERSION = 1\n",
    "ROOT_DIR = '../../vehicle_dataset_v{}'.format(DATASET_VERSION)\n",
    "IMAGES_DIR = os.path.join(ROOT_DIR, 'images')\n",
    "ANNOTATIONS_DIR = os.path.join(ROOT_DIR, 'annotations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize new dir tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(IMAGES_DIR)\n",
    "    os.makedirs(ANNOTATIONS_DIR)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crawl dirs and rename csv files, images to include original image name\n",
    "### move all pics to 'images' dir and all csvs to 'annotations' dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 842 files.\n"
     ]
    }
   ],
   "source": [
    "move_list=[]\n",
    "empty_folders=[]\n",
    "for root,_,files in os.walk(ROOT_DIR):\n",
    "    if root==IMAGES_DIR or root==ANNOTATIONS_DIR or len(files)==0:\n",
    "        continue\n",
    "    source_image_name = root.split(os.sep)[-1].replace('crop','')\n",
    "    for file in files:\n",
    "        src = os.path.join(root,file)\n",
    "        if file == 'Images.csv':\n",
    "            dst = os.path.join(ANNOTATIONS_DIR,source_image_name+'.csv')\n",
    "        else:\n",
    "            patch_id = int(os.path.splitext(file)[0].replace('pic',''))\n",
    "            new_filename = '{}_{:05}.jpg'.format(source_image_name, patch_id)\n",
    "            dst = os.path.join(IMAGES_DIR, new_filename)\n",
    "        move_list.append((src,dst))\n",
    "    empty_folders.append(root)\n",
    "\n",
    "# move pics and csvs\n",
    "for src, dst in move_list:\n",
    "    shutil.move(src,dst)\n",
    "print 'Moved {} files.'.format(len(move_list))\n",
    "# del empty folders\n",
    "for fldr in empty_folders:\n",
    "    shutil.rmtree(fldr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## each large sweep was cut into smaller pictures, these pictures are called 'patches' from here on out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ('__background__',\n",
    "           'Car', 'Van', 'Truck',\n",
    "           'ConcreteTruck', 'Bus')\n",
    "num_classes = len(classes)\n",
    "class_to_ind = dict(zip(classes, xrange(num_classes)))\n",
    "\n",
    "def correct_alignment(raw_points):\n",
    "    raw_points[raw_points < 1] = 1\n",
    "    raw_points[raw_points > 900] = 900\n",
    "    return\n",
    "\n",
    "def get_shapes(patch_df):\n",
    "    raw_points = np.array(patch_df.ix[:,1:], dtype=np.float32)[:,::-1]\n",
    "    correct_alignment(raw_points)\n",
    "    polygons = np.zeros((raw_points.shape[0],4,2), dtype=np.float32)\n",
    "    bboxes = np.zeros((raw_points.shape[0],4), dtype=np.float32)\n",
    "    for i in xrange(raw_points.shape[0]):\n",
    "        poly = raw_points[i].reshape(4,2) - 1 # zero-index points\n",
    "        polygons[i]=poly\n",
    "        bboxes[i,0:2]=np.min(poly, 0) # get xmin, ymin\n",
    "        bboxes[i,2:4]=np.max(poly, 0) # get xmax, ymax\n",
    "    return bboxes, polygons\n",
    "\n",
    "def get_classes(patch_df):\n",
    "    return patch_df['Entities EntityType'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the following script parses this 'patch db' from all original csv files using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "csvs = glob(os.path.join(ANNOTATIONS_DIR, '*.csv'))\n",
    "patch_db={}\n",
    "s=set()\n",
    "\n",
    "for csv in csvs:\n",
    "    source_image_name = os.path.basename(os.path.splitext(csv)[0])\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # weed out all invalid boxes per patch\n",
    "    patch_list = df['FileName']\n",
    "    i = 0\n",
    "    while i<len(patch_list) and isinstance(patch_list[i],str):\n",
    "        j=1\n",
    "        while i+j<len(patch_list) and not isinstance(patch_list[i+j],str):\n",
    "            j+=1\n",
    "        \n",
    "        # set flattened patch name\n",
    "        patch_id = int(os.path.splitext(patch_list[i])[0].replace('pic',''))\n",
    "        patch_name = '{}_{:05}.jpg'.format(source_image_name, patch_id)\n",
    "        patch_df = df.ix[i:i+j-1, 'Entities EntityType':'Entities P1 X'].reset_index(drop=True)\n",
    "        first_entry = patch_df.ix[0,'Entities EntityType']\n",
    "        \n",
    "        # log bboxes and polygons in patch to patch db. skip non-existing pics and patches with no labels.\n",
    "        if isinstance(first_entry,str) and os.path.exists(os.path.join(IMAGES_DIR,patch_name)):\n",
    "            bboxes, polygons = get_shapes(patch_df)\n",
    "            gt_classes = get_classes(patch_df)\n",
    "            patch_db[patch_name]= {'boxes' : bboxes,\n",
    "                                   'polygons' : polygons,\n",
    "                                   'gt_classes' : gt_classes}\n",
    "        # advance loop\n",
    "        i+=j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached end\n"
     ]
    }
   ],
   "source": [
    "# check for nans\n",
    "for _,d in patch_db.iteritems():\n",
    "    for k,v in d.iteritems():\n",
    "        if k == 'boxes' or k == 'polygons':\n",
    "            if v is not None and (np.isnan(v).any() == True or np.isnan(v).any() == True):\n",
    "                print('found nan in: ')\n",
    "                print(k,v)\n",
    "print 'reached end'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-empty patches: 663\n",
      "patches total: 663\n"
     ]
    }
   ],
   "source": [
    "# non-empty patches\n",
    "nep = [k for k in patch_db.keys() if patch_db[k]['boxes'] is not None]\n",
    "print 'non-empty patches: {}'.format(len(nep))\n",
    "# patches total\n",
    "print 'patches total: {}'.format(len(patch_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save to disk\n",
    "with open(os.path.join(ANNOTATIONS_DIR, 'complete_dataset_v{}.pkl'.format(DATASET_VERSION)), 'wb') as f:\n",
    "    pkl.dump(patch_db, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize bboxes in patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import cPickle as pkl\n",
    "\n",
    "DATASET_VERSION = 1\n",
    "ROOT_DIR = '../../vehicle_dataset_v{}'.format(DATASET_VERSION)\n",
    "IMAGES_DIR = os.path.join(ROOT_DIR, 'images')\n",
    "ANNOTATIONS_DIR = os.path.join(ROOT_DIR, 'annotations')\n",
    "\n",
    "def vis_detections(im, title, roidb): #tp, fp, fn):\n",
    "    \"\"\"Visual debugging of detections.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    im = im[:, :, (2, 1, 0)]\n",
    "    plt.cla()\n",
    "    plt.title(title)\n",
    "    plt.imshow(im)\n",
    "    _draw_detections(im, roidb, plt)\n",
    "    plt.show()\n",
    "\n",
    "def _draw_detections(im, roidb, plt):\n",
    "    boxes = roidb['boxes']\n",
    "    if boxes is None: return\n",
    "    for i in xrange(boxes.shape[0]):\n",
    "        bbox = boxes[i, :4]\n",
    "        plt.gca().add_patch(\n",
    "            plt.Rectangle((bbox[0], bbox[1]),\n",
    "                          bbox[2] - bbox[0],\n",
    "                          bbox[3] - bbox[1], fill=False,\n",
    "                          edgecolor='red', linewidth=2)\n",
    "            )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load patch_db\n",
    "with open(os.path.join(ANNOTATIONS_DIR, 'complete_dataset_v{}.pkl'.format(DATASET_VERSION))) as f:\n",
    "    patch_db = pkl.load(f)\n",
    "\n",
    "# visualize annotations\n",
    "for k,v in patch_db.iteritems():\n",
    "    image_path = os.path.join(IMAGES_DIR, k)\n",
    "    im = cv2.imread(image_path)\n",
    "    if im is None:\n",
    "        print(\"couldn't load image: {}\".format(image_path))\n",
    "        continue\n",
    "#     print(image_path)\n",
    "#     vis_detections(im, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split patch_db into train,val,test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.cross_validation as cv\n",
    "\n",
    "ss = cv.ShuffleSplit(len(patch_db), n_iter=1, test_size=0.15)\n",
    "for t1, t2 in ss:\n",
    "    ss2 = cv.ShuffleSplit(len(t1), n_iter=1, test_size=0.15)\n",
    "    for r1, r2 in ss2:\n",
    "        train = t1[r1]\n",
    "        val = t1[r2]\n",
    "    test = t2\n",
    "\n",
    "with open(os.path.join(ANNOTATIONS_DIR, 'splits_indices_v{}.pkl'.format(DATASET_VERSION)), 'wb') as f:\n",
    "    pkl.dump({'train': train,\n",
    "              'val': val,\n",
    "              'test': test}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_split_from_ds(ds, idx):\n",
    "    split = {}\n",
    "    keys = ds.keys()\n",
    "    for j in xrange(len(idx)):\n",
    "        k = keys[idx[j]]\n",
    "        split[k] = patch_db[k]\n",
    "    return split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_file = os.path.join(ANNOTATIONS_DIR, 'complete_dataset_v{}.pkl'.format(DATASET_VERSION))\n",
    "splits_file = os.path.join(ANNOTATIONS_DIR, 'splits_indices_v{}.pkl'.format(DATASET_VERSION))\n",
    "\n",
    "if os.path.exists(dataset_file):\n",
    "    with open(dataset_file) as f:\n",
    "        patch_db = pkl.load(f)\n",
    "\n",
    "    if os.path.exists(splits_file):\n",
    "        # load splits indices\n",
    "        with open(splits_file) as f:\n",
    "            d = pkl.load(f)\n",
    "            \n",
    "        train_ds = get_split_from_ds(patch_db, d['train'])\n",
    "        val_ds = get_split_from_ds(patch_db, d['val'])\n",
    "        test_ds = get_split_from_ds(patch_db, d['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get stats from splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_boxes(ds):\n",
    "    count = 0\n",
    "    for k,v in ds.iteritems():\n",
    "        count += len(v['gt_classes'])\n",
    "    return len(ds), count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats for vehicle dataset, version: 1\n",
      "-------\n",
      "train patches: 478, vehicles: 1842\n",
      "val patches: 85, vehicles: 347\n",
      "test patches: 100, vehicles: 349\n"
     ]
    }
   ],
   "source": [
    "print('stats for vehicle dataset, version: {}\\n-------'.format(DATASET_VERSION))\n",
    "print('train patches: {}, vehicles: {}'.format(*count_boxes(train_ds)))\n",
    "print('val patches: {}, vehicles: {}'.format(*count_boxes(val_ds)))\n",
    "print('test patches: {}, vehicles: {}'.format(*count_boxes(test_ds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
